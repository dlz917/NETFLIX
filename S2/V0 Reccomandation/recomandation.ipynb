{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Systeme de Recommandation V0\n",
    "# Version 1 User\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\joans\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\joans\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\joans\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\joans\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recuperer la requete de filtrage et lutilsateur\n",
    "\n",
    "user=sys.argv[1]\n",
    "requete=sys.argv[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pour recuperer la listes des films faut avant recuperer tous les participants de la salle (liste de id de utilisateurs)\n",
    "historique = \"select id_show from regarder where id_us = \" + str(user) + \";\"\n",
    "\n",
    "#recuperer la liste des films dans l'historique de l'utilisateur1(liste de id)\n",
    "#recuperer la liste des films dans l'historique de l'utilisateur2(liste de id)\n",
    "#recuperer la liste des films dans l'historique de l'utilisateur3(liste de id)\n",
    "#recuperer la liste des films dans l'historique de l'utilisateur4(liste de id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pymysql.connections.Connection object at 0x000001D241B5BCD0>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pymysql\n",
    "import pandas as pd\n",
    "# Some other example server values are\n",
    "# server = 'localhost\\sqlexpress' # for a named instance\n",
    "# server = 'myserver,port' # to specify an alternate port\n",
    "cnxn = pymysql.connect(user=\"nchoice\", password=\"jenesaispas\", host=\"localhost\", database=\"nchoice\")\n",
    "\n",
    "print(cnxn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joans\\anaconda3\\lib\\site-packages\\pandas\\io\\sql.py:762: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#t = historique['id_show'].tolist()\n",
    "t=(\"s3\",\"s8\",\"s10\") #TEST\n",
    "requete_historique = \"SELECT show_new.*, GROUP_CONCAT(DISTINCT jouer.id_cast SEPARATOR ',') as cast_ids, GROUP_CONCAT(DISTINCT produire.id_direc SEPARATOR ',') as director_ids,GROUP_CONCAT(DISTINCT etre.id_genre SEPARATOR ',') as genre_ids FROM show_new LEFT JOIN jouer ON jouer.id_show = show_new.id_show LEFT JOIN produire ON produire.id_show = show_new.id_show LEFT JOIN etre ON etre.id_show = show_new.id_show  WHERE show_new.id_show IN {} GROUP BY show_new.id_show; \".format(t)\n",
    "\n",
    "films_historiques = pd.read_sql(requete_historique, cnxn)\n",
    "#films_filtres = pd.read_sql(requete, cnxn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for every film in the list of films that passed the manual filtering, get the correlation between the film and the films in the historial (1 for user) (4 max) (id)\n",
    "# faire la moyenne donc chaque user il aura un correspondance avec chaque film qui passe le filtre manuel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_similarity(list1, list2):\n",
    "    if len(list1) == 0 or len(list2) == 0:\n",
    "        return 0\n",
    "    set1 = set(map(tuple, list1))\n",
    "    set2 = set(map(tuple, list2))\n",
    "\n",
    "    intersection = set1.intersection(set2)\n",
    "    union = set1.union(set2)\n",
    "\n",
    "    similarity = len(intersection) / len(union) \n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_text = [word for word in text.lower().split() if word not in stop_words]\n",
    "    \n",
    "    # Perform stemming\n",
    "    ps = PorterStemmer()\n",
    "    stemmed_text = [ps.stem(word) for word in filtered_text]\n",
    "    \n",
    "    # Perform lemmatization\n",
    "    lem = WordNetLemmatizer()\n",
    "    lemmatized_text = [lem.lemmatize(word, \"v\") for word in stemmed_text]\n",
    "    \n",
    "    return \" \".join(lemmatized_text)\n",
    "\n",
    "def calculate_similarity(filtres, historique):\n",
    "    genre_weight = 0.2\n",
    "    director_weight = 0.15\n",
    "    cast_weight = 0.20\n",
    "    type_weight = 0.05\n",
    "    description_weight = 0.3\n",
    "    year_weight = 0.05\n",
    "    country_weight = 0.05\n",
    "    # Create a copy of the dataframes\n",
    "    filtres_similarity = filtres.copy()\n",
    "    historique_similarity = historique.copy()\n",
    "    \n",
    "    #Preprocess the  lists columns\n",
    "    filtres_similarity[\"cast_ids\"]=filtres_similarity[\"cast_ids\"].str.split(\",\")\n",
    "    filtres_similarity[\"director_ids\"]=filtres_similarity[\"director_ids\"].str.split(\",\")\n",
    "    filtres_similarity[\"genre_ids\"]=filtres_similarity[\"genre_ids\"].str.split(\",\")\n",
    "    filtres_similarity[\"country\"]=filtres_similarity[\"country\"].str.split(\",\")\n",
    "\n",
    "    historique_similarity[\"cast_ids\"]=historique_similarity[\"cast_ids\"].str.split(\",\")\n",
    "    historique_similarity[\"director_ids\"]=historique_similarity[\"director_ids\"].str.split(\",\")\n",
    "    historique_similarity[\"genre_ids\"]=historique_similarity[\"genre_ids\"].str.split(\",\")\n",
    "    historique_similarity[\"country\"]=historique_similarity[\"country\"].str.split(\",\")\n",
    "\n",
    "    # Preprocess the description column\n",
    "    filtres_similarity['description'] = filtres_similarity['description'].apply(preprocess_text)\n",
    "    historique_similarity['description'] = historique_similarity['description'].apply(preprocess_text)\n",
    "\n",
    "    #Acces rows of the filtered films dataframe and calculate the similarity between every filtered film and the films in the historial\n",
    "    df_similarity = pd.DataFrame(columns=['id_show', 'similarity'])\n",
    "    for index, row in filtres_similarity.iterrows():\n",
    "        similarity=0\n",
    "        for index2, row2 in historique_similarity.iterrows():\n",
    "            if row['type']==row2['type']:\n",
    "                similarity+=1*type_weight\n",
    "            if row['year']==row2['year']:\n",
    "                similarity+=1*year_weight\n",
    "            similarity+=list_similarity(row['genre_ids'],row2['genre_ids'])*genre_weight\n",
    "            similarity+=list_similarity(row['director_ids'],row2['director_ids'])*director_weight\n",
    "            similarity+=list_similarity(row['cast_ids'],row2['cast_ids'])*cast_weight\n",
    "            similarity+=list_similarity(row['country'],row2['country'])*country_weight\n",
    "            vectorizer = CountVectorizer()\n",
    "            # Transform the descriptions into vectors\n",
    "            vectors = vectorizer.fit_transform([row[\"description\"], row2[\"description\"]])\n",
    "            # Calculate the cosine similarity between the two vectors\n",
    "            cosine_sim = cosine_similarity(vectors[0], vectors[1])[0][0]\n",
    "            similarity+=cosine_sim*description_weight\n",
    "\n",
    "        new_row={'id_show':row['id_show'],'similarity':similarity/len(historique_similarity)}\n",
    "        df_similarity=df_similarity.append(new_row, ignore_index=True)\n",
    "\n",
    "   \n",
    "\n",
    "    return df_similarity.sort_values(by=['similarity'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joans\\anaconda3\\lib\\site-packages\\pandas\\io\\sql.py:762: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "C:\\Users\\joans\\AppData\\Local\\Temp\\ipykernel_3688\\3096245369.py:75: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_similarity=df_similarity.append(new_row, ignore_index=True)\n",
      "C:\\Users\\joans\\AppData\\Local\\Temp\\ipykernel_3688\\3096245369.py:75: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_similarity=df_similarity.append(new_row, ignore_index=True)\n",
      "C:\\Users\\joans\\AppData\\Local\\Temp\\ipykernel_3688\\3096245369.py:75: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_similarity=df_similarity.append(new_row, ignore_index=True)\n",
      "C:\\Users\\joans\\AppData\\Local\\Temp\\ipykernel_3688\\3096245369.py:75: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_similarity=df_similarity.append(new_row, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_show</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s812</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s1010</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s9</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s333</td>\n",
       "      <td>0.044048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id_show  similarity\n",
       "2    s812    0.083333\n",
       "0   s1010    0.050000\n",
       "3      s9    0.050000\n",
       "1    s333    0.044048"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TEST\n",
    "t=(\"s9\",\"s812\",\"s1010\",\"s333\")\n",
    "requete= \"SELECT show_new.*, GROUP_CONCAT(DISTINCT jouer.id_cast SEPARATOR ',') as cast_ids, GROUP_CONCAT(DISTINCT produire.id_direc SEPARATOR ',') as director_ids,GROUP_CONCAT(DISTINCT etre.id_genre SEPARATOR ',') as genre_ids FROM show_new LEFT JOIN jouer ON jouer.id_show = show_new.id_show LEFT JOIN produire ON produire.id_show = show_new.id_show LEFT JOIN etre ON etre.id_show = show_new.id_show  WHERE show_new.id_show IN {} GROUP BY show_new.id_show; \".format(t)\n",
    "films_filtres = pd.read_sql(requete, cnxn)\n",
    "films_filtres\n",
    "films_filtres = films_filtres.astype(str)\n",
    "films_historiques = films_historiques.astype(str)\n",
    "calculate_similarity(films_filtres, films_historiques)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version plusieurs utilisateurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "75fc30232918a5324d2f6606dfe0fdff9096202f58b1090ad0b1a4a6fd7a6572"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
